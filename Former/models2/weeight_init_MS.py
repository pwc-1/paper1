import math
import re
import warnings
import numpy as np
import mindspore
import mindspore.ops as ops
import mindspore.nn as nn
import mindspore.ops.operations as P
import mindspore.common.initializer as weight_init
import mindspore.common.dtype as mstype
from mindspore import load_checkpoint, load_param_into_net
from einops import repeat


def show_state_dict(state_dict):
	for name, value in state_dict.items():
		print(name)

def replace_state_dict(state_dict):
	for old_key in list(state_dict.keys()):
		if old_key.startswith('model'):
			new_key = old_key[6:] # skip 'model.'
			if 'in_proj' in new_key:
				new_key = new_key.replace('in_proj_', 'qkv.') #in_proj_weight -> qkv.weight
			elif 'out_proj' in new_key:
				new_key = new_key.replace('out_proj', 'proj')
			state_dict[new_key] = state_dict.pop(old_key)
		else: # cls_head
			new_key = old_key[9:]
			state_dict[new_key] = state_dict.pop(old_key)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    def norm_cdf(x):
        return (1. + ops.erf(x / ops.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor_init = np.random.uniform(2 * l - 1, 2 * u - 1, tensor.shape).astype(np.float32)
    tensor.set_data(tensor_init)

    ## Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor = ops.erfinv(tensor)

    # Transform to proper mean, std
    tensor = tensor * (std * ops.sqrt(2.))
    tensor = tensor + mean

    # Clamp to ensure it's in the proper range
    tensor = mindspore.ops.clip_by_value(tensor, a, b)
    return tensor




def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    ## type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

    


def constant_init_(tensor, constant_value=0):
    tensor.set_data(weight_init.initializer(weight_init.Constant(), tensor.shape))
    return tensor

def kaiming_init_(tensor,
                  a=0,
                  mode='fan_out',
                  nonlinearity='relu',
                  distribution='normal'):
    assert distribution in ['uniform', 'normal']
    if distribution == 'uniform':
        tensor.set_data(weight_init.initializer(weight_init.HeUniform(mode=mode, nonlinearity=nonlinearity), tensor.shape))
    else:
        tensor.set_data(weight_init.initializer(weight_init.HeNormal(mode=mode, nonlinearity=nonlinearity), tensor.shape))
    return tensor


def init_from_vit_pretrain_(module,
                             pretrained,
                             conv_type,
                             attention_type,
                             copy_strategy,
                             extend_strategy='temporal_avg',
                             tube_size=2,
                             num_time_transformer_layers=4):

    if isinstance(pretrained, str):
        state_dict = load_checkpoint(pretrained)
        
        old_state_dict_keys = list(state_dict.keys())
        for old_key in old_state_dict_keys:
            # extend the Conv2d params to Conv3d
            if conv_type == 'Conv3d':
                if 'patch_embed.projection.weight' in old_key:
                    weight = state_dict[old_key]
                    new_weight = weight.expand(1, tube_size, -1, -1, -1)
                    if extend_strategy == 'temporal_avg':
                        new_weight = new_weight / tube_size
                    elif extend_strategy == 'center_frame':
                        new_weight.zero_()
                        new_weight[:,:,tube_size//2,:,:] = weight
                    state_dict[old_key] = new_weight
                    continue
                    
            # modify the key names of norm layers
            if attention_type == 'fact_encoder':
                new_key = old_key.replace('transformer_layers.layers',
                                          'transformer_layers.0.layers')
            else:
                new_key = old_key
            
            if 'in_proj' in new_key:
                new_key = new_key.replace('in_proj_', 'qkv.') #in_proj_weight -> qkv.weight
            elif 'out_proj' in new_key:
                new_key = new_key.replace('out_proj', 'proj')

            if 'norms' in new_key:
                new_key = new_key.replace('norms.0', 'attentions.0.norm')
                new_key = new_key.replace('norms.1', 'ffns.0.norm')

            state_dict[new_key] = state_dict.pop(old_key)

        old_state_dict_keys = list(state_dict.keys())
        for old_key in old_state_dict_keys:
            # copy the parameters of space attention to time attention
            if attention_type == 'divided_space_time':
                if 'attentions.0' in old_key:
                    new_key = old_key.replace('attentions.0',
                                              'attentions.1')
                    if copy_strategy == 'repeat':
                        state_dict[new_key] = state_dict[old_key].clone()
                    elif copy_strategy == 'set_zero':
                        state_dict[new_key] = state_dict[old_key].clone().zero_()
            # copy the part of parameters of space attention to time attention
            elif attention_type == 'fact_encoder':
                pattern = re.compile(r'(?<=layers.)\d+')
                matchObj = pattern.findall(old_key)
                if len(matchObj) > 1 and int(matchObj[1]) < num_time_transformer_layers:
                    new_key = old_key.replace('transformer_layers.0.layers', 
                                              'transformer_layers.1.layers')
                    if copy_strategy == 'repeat':
                        state_dict[new_key] = state_dict[old_key].clone()
                    elif copy_strategy == 'set_zero':
                        state_dict[new_key] = state_dict[old_key].clone().zero_()

        missing_keys,unexpected_keys = module.load_state_dict(state_dict, strict=False)


def init_from_mae_pretrain_(module,
                            pretrained,
                            conv_type,
                            attention_type,
                            copy_strategy,
                            extend_strategy='temporal_avg',
                            tube_size=2,
                            num_time_transformer_layers=4):
    if isinstance(pretrained, str):
        state_dict = load_checkpoint(pretrained)
        old_state_dict_keys = list(state_dict.keys())
        for old_key in old_state_dict_keys:
            if 'decoder' in old_key:
                state_dict.pop(old_key)
                continue

            if 'encoder.patch_embed.proj' in old_key:
                new_key = old_key.replace('encoder.patch_embed.proj',
                                          'patch_embed.projection')
                if conv_type == 'Conv3d' and 'weight' in old_key:
                    weight = ops.Tensor(state_dict[old_key].asnumpy(), dtype=state_dict[old_key].dtype())
                    new_weight = weight.repeat(0, 0, tube_size, 0, 0)
                    if extend_strategy == 'temporal_avg':
                        new_weight = new_weight / tube_size
                    elif extend_strategy == 'center_frame':
                        new_weight.zero_()
                        new_weight[:, :, tube_size // 2, :, :] = weight
                    state_dict.pop(old_key)
                    state_dict[new_key] = new_weight
                else:
                    state_dict[new_key] = state_dict.pop(old_key)
                continue

            if attention_type == 'fact_encoder':
                new_key = old_key.replace('encoder.blocks', 'transformer_layers.0.layers')
            else:
                new_key = old_key.replace('encoder.blocks', 'transformer_layers.layers')

            if 'norm' in new_key:
                new_key = new_key.replace('norm1', 'attentions.0.norm')
                new_key = new_key.replace('norm2', 'ffns.0.norm')
            elif 'attn' in new_key:
                if 'q_bias' in new_key:
                    pattern = re.compile(r'(?<=blocks.)\d+')
                    matchObj = pattern.findall(old_key)
                    block_id = int(matchObj[0])
                    q_bias = state_dict[f'encoder.blocks.{block_id}.attn.q_bias']
                    v_bias = state_dict[f'encoder.blocks.{block_id}.attn.v_bias']
                    weight = ops.Concat((q_bias, ops.ZerosLike(q_bias), v_bias))
                    new_key = new_key.replace('attn.q_bias', 'attentions.0.attn.qkv.bias')
                    state_dict.pop(f'encoder.blocks.{block_id}.attn.q_bias')
                    state_dict.pop(f'encoder.blocks.{block_id}.attn.v_bias')
                    state_dict[new_key] = weight
                    continue
                elif 'v_bias' in new_key:
                    continue
            elif 'mlp' in new_key:
                new_key = new_key.replace('mlp.fc1', 'ffns.0.layers.0.0')
                new_key = new_key.replace('mlp.fc2', 'ffns.0.layers.1')

            if 'encoder.norm' in old_key:
                new_key = old_key.replace('encoder.norm', 'norm')

            state_dict[new_key] = state_dict.pop(old_key)

        old_state_dict_keys = list(state_dict.keys())
        for old_key in old_state_dict_keys:
            if attention_type == 'divided_space_time':
                if 'attentions.0' in old_key:
                    new_key = old_key.replace('attentions.0', 'attentions.1')
                    if copy_strategy == 'repeat':
                        state_dict[new_key] = state_dict[old_key].clone()
                    elif copy_strategy == 'set_zero':
                        state_dict[new_key] = state_dict[old_key].clone().zero_()
            elif attention_type == 'fact_encoder':
                pattern = re.compile(r'(?<=layers.)\d+')
                matchObj = pattern.findall(old_key)
                if len(matchObj) > 1 and int(matchObj[1]) < num_time_transformer_layers:
                    new_key = old_key.replace('transformer_layers.0.layers', 'transformer_layers.1.layers')
                    if copy_strategy == 'repeat':
                        state_dict[new_key] = state_dict[old_key].clone()
                    elif copy_strategy == 'set_zero':
                        state_dict[new_key] = state_dict[old_key].clone().zero_()

        load_param_into_net(module, state_dict)


     
def init_from_kinetics_pretrain_(module, pretrained, init_module=None):
    state_dict = load_checkpoint(pretrained)
    if 'state_dict' in state_dict:
        state_dict = state_dict['state_dict']

    # meaningful?
    replace_state_dict(state_dict)
    '''
    if init_module == 'transformer':
        replace_state_dict(state_dict)
    elif init_module == 'cls_head':
        replace_state_dict(state_dict)
    else:
        raise TypeError(f'pretrained weights do not include the {init_module} module')
    '''
    load_param_into_net(module, state_dict)